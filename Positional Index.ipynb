{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from natsort import natsorted\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Heading and footer of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header_footer(final_string):\n",
    "    new_final_string = \"\"\n",
    "    tokens = final_string.split('\\n')\n",
    "  \n",
    "    # Remove tokens[0] and tokens[-1]\n",
    "    for token in tokens[1:-1]:\n",
    "        new_final_string += token+\" \"\n",
    "        \n",
    "    return new_final_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding =\"ascii\", errors =\"surrogateescape\") as f:\n",
    "        stuff = f.read()\n",
    "  \n",
    "    f.close()\n",
    "      \n",
    "    # Remove header and footer.\n",
    "    stuff = remove_header_footer(stuff)\n",
    "      \n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuations and lower casing and setemming the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(final_string):\n",
    "        # Tokenize.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    token_list = tokenizer.tokenize(final_string)\n",
    " \n",
    "    # Remove punctuations.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    token_list = [word.translate(table) for word in token_list]\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in token_list]\n",
    "    token_list = [str for str in stripped_words if str]\n",
    " \n",
    "    # Change to lowercase.\n",
    "    token_list =[word.lower() for word in token_list]\n",
    "    return token_list\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thi': [6, {0: [0], 1: [0], 2: [4, 13, 17], 3: [4]}],\n",
       " 'is': [8, {0: [1], 1: [1], 2: [2, 5, 14, 18], 3: [2, 5]}],\n",
       " 'the': [5, {0: [2], 1: [2], 2: [6, 15], 3: [6]}],\n",
       " 'second': [4, {0: [3], 1: [3], 2: [7], 3: [7]}],\n",
       " 'text': [4, {0: [4], 1: [4], 2: [8], 3: [8]}],\n",
       " 'hello': [4, {0: [5], 1: [5], 2: [9], 3: [9]}],\n",
       " 'how': [4, {0: [6], 1: [6], 2: [10], 3: [10]}],\n",
       " 'are': [4, {0: [7], 1: [7], 2: [11], 3: [11]}],\n",
       " 'you': [4, {0: [8], 1: [8], 2: [12], 3: [12]}],\n",
       " 'my': [2, {2: [0], 3: [0]}],\n",
       " 'name': [2, {2: [1], 3: [1]}],\n",
       " 'shayanth': [2, {2: [3], 3: [3]}],\n",
       " 'third': [1, {2: [16]}],\n",
       " 'it': [1, {2: [19]}],\n",
       " 'now': [1, {2: [20]}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_names = [\"files\"]\n",
    " \n",
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    " \n",
    "# Initialize the dictionary.\n",
    "pos_index = {}\n",
    " \n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}\n",
    " \n",
    "for folder_name in folder_names:\n",
    " \n",
    "    # Open files.\n",
    "    file_names = natsorted(os.listdir(\"./\" + folder_name))\n",
    " \n",
    "    # For every file.\n",
    "    for file_name in file_names:\n",
    " \n",
    "        # Read file contents.\n",
    "        stuff = read_file(\"./\" + folder_name + \"/\" + file_name)\n",
    "         \n",
    "        # This is the list of words in order of the text.\n",
    "        # We need to preserve the order because we require positions.\n",
    "        # 'preprocessing' function does some basic punctuation removal,\n",
    "        # stopword removal etc.\n",
    "\n",
    "        final_token_list = preprocessing(stuff)\n",
    " \n",
    "        # For position and term in the tokens.\n",
    "        for pos, term in enumerate(final_token_list):\n",
    "             \n",
    "                    # First stem the term.\n",
    "                    term = stemmer.stem(term)\n",
    " \n",
    "                    # If term already exists in the positional index dictionary.\n",
    "                    if term in pos_index:\n",
    "                         \n",
    "                        # Increment total freq by 1.\n",
    "                        pos_index[term][0] = pos_index[term][0] + 1\n",
    "                         \n",
    "                        # Check if the term has existed in that DocID before.\n",
    "                        if fileno in pos_index[term][1]:\n",
    "                            pos_index[term][1][fileno].append(pos)\n",
    "                             \n",
    "                        else:\n",
    "                            pos_index[term][1][fileno] = [pos]\n",
    " \n",
    "                    # If term does not exist in the positional index dictionary\n",
    "                    else:\n",
    "                         \n",
    "                        # Initialize the list.\n",
    "                        pos_index[term] = []\n",
    "                        # The total frequency is 1.\n",
    "                        pos_index[term].append(1)\n",
    "                        # The postings list is initially empty.\n",
    "                        pos_index[term].append({})     \n",
    "                        # Add doc ID to postings list.\n",
    "                        pos_index[term][1][fileno] = [pos]\n",
    " \n",
    "        # Map the file no. to the file name.\n",
    "        file_map[fileno] = \"./\" + folder_name + \"/\" + file_name\n",
    " \n",
    "        # Increment the file no. counter for document ID mapping             \n",
    "        fileno += 1\n",
    "pos_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################\n",
      "Positional Index\n",
      "frequency:2\n",
      "################\n",
      "Flie Location: ./files/file4.txt\n",
      " Position of query[3]\n",
      "################\n",
      "\n",
      "Flie Location: ./files/file.txt\n",
      " Position of query[3]\n",
      "################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pos_index[\"shayanth\"]\n",
    "print(\"################\")\n",
    "print(\"Positional Index\")\n",
    "print(\"frequency:\"+str(test[0]))\n",
    "print(\"################\")\n",
    " \n",
    "file_list = test[1]\n",
    "\n",
    "for fileno, positions in file_list.items():\n",
    "    print(\"Flie Location: \"+str(file_map[fileno] )+\"\\n Position of query\"+str( positions)+\"\\n################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d0dc8c7fe6b6d64df00d937a6bb48ad8aa09ae06976035b169f81746ce1e3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
